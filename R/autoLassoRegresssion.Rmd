---
title: "autoLassoRegresssion"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{autoLassoRegresssion}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(TopK)
```
## Introduction

The `autoLassoRegression` function fits a Lasso regression model to a given dataset, with optional bagging to improve model stability and robustness. Lasso regression is a type of linear regression that uses L1 regularization to encourage sparsity, potentially leading to models with fewer predictors. This vignette demonstrates how to use the function with a focus on bagging, including detailed explanations and examples.

## Function Overview

The `autoLassoRegression` function can fit a Lasso regression model with or without bagging. Bagging (Bootstrap Aggregating) creates multiple bootstrapped samples, fits separate models to each, and combines results to generate a more robust outcome. This approach helps to reduce variance and increase the reliability of the model's predictions.

## Parameters

- **X**: A data frame or matrix containing the predictor variables.
- **y**: A vector representing the response variable. It can be binary or continuous.
- **lambda**: The Lasso penalty parameter. If `NULL`, it will be computed using cross-validation.
- **family**: The type of response variable, `"binary"` for logistic regression or `"gaussian"` for linear regression.
- **bagging**: Logical indicating whether to use bagging (default is `FALSE`).
- **n_bags**: The number of bootstrapped samples for bagging (default is `100`).

## Handling Missing Data

The function checks for missing data in `X` and `y`. If there are missing values, the corresponding rows are removed, ensuring a clean dataset for model fitting.


```{r}
# Example data with missing values
X <- data.frame(
  x1 = c(1, 2, NA, 4, 5),
  x2 = c(6, NA, 8, 9, 10)
)
y <- c(1, 0, 1, 0, NA)
# Applying autoLassoRegression
result <- autoLassoRegression(X, y, bagging = FALSE)

print("Number of rows after handling missing data:")
print(nrow(result$model$X))
```

## Bagging Process

If bagging = TRUE, the function creates multiple bootstrapped samples and fits a lasso regression model to each. This process allows you to assess the variability of coefficients across models and derive importance scores to identify which predictors are consistently selected.

**Bootstrapped Samples:** Given a dataset with n observations, a bootstrapped sample is created by selecting n observations with replacement. This process allows some observations to be selected multiple times, while others might not be selected at all.

**Fitting Models:** Once you have a set of bootstrapped samples, a model (in this case, lasso regression) is fitted to each sample. Because each sample is different due to random sampling, the resulting models can vary, reflecting different perspectives on the underlying data.

**Combining Results:** After fitting models to multiple bootstrapped samples, the results are combined to create a more robust output. This combining process is central to bagging, where the goal is to reduce variance by leveraging the diversity of the bootstrapped samples.


## Combining Results from Bagged Models

In the autoLassoRegression function, results from bagged models are combined as follows:

Averaging Predictions:
Each bagged model makes predictions on the original dataset.
The final predictions are derived by averaging (rowMeans()) the predictions from all bagged models.
Calculating Importance Scores:
Importance scores represent the number of times each predictor is selected in the bagging process. A higher score indicates a predictor is consistently chosen in most models, suggesting its importance.
These scores help identify key predictors and can be used to guide feature selection.

## Interpretation and Usage
When using bagging in `autoLassoRegression`, the key benefits are:

1. **Reduced Variance**: By averaging predictions across multiple bagged Lasso models, you reduce the impact of individual model variance, leading to more robust results.

2. **Increased Stability**: Bagging with Lasso creates more stable outcomes by reducing the influence of outlier models, contributing to consistent predictions.

3. **Understanding Predictor Importance**: Bagging allows you to estimate importance scores by counting how many times each predictor is selected across all bagged models. This information helps identify which predictors are most important and guides feature selection.

```{r}

# Example data
set.seed(123)
X <- data.frame(
  x1 = rnorm(100),
  x2 = rnorm(100),
  x3 = rnorm(100)
)
y <- sample(c(0, 1), 100, replace = TRUE)  # Binary response

# Fit Lasso regression with bagging
result_bagging <- autoLassoRegression(X, y, bagging = TRUE, n_bags = 100)

# Display importance scores
print("Importance scores for predictors:")
print(result_bagging$importance_scores)

# Print final predictions
print("Final predictions from bagged models:")
print(result_bagging$predictions)
```


### Combining Results from Bagged Models
In autoLassoRegression, when bagging is enabled, multiple bootstrapped samples are created and a Lasso regression model is fitted to each. The results are then combined to create a final prediction and importance scores.

**Averaging Predictions:**
The final predictions are derived by averaging (rowMeans()) the predictions from all bagged models. This approach reduces individual model variance, leading to more stable predictions.

**Importance Scores:**

The importance scores represent the number of times each predictor is selected across all bagged models. This information can help identify key predictors and guide feature selection in Lasso regression, where variable selection is crucial.


```{r}
# Example with 100 bootstrapped samples
result_bagging <- autoLassoRegression(X, y, bagging = TRUE, n_bags = 100)

# Display averaged predictions from all bagged models
print("Final predictions from bagged models:")
print(result_bagging$predictions)

```

## Final Thoughts

The autoLassoRegression function provides a straightforward way to fit Lasso regression with an optional bagging approach. This vignette has demonstrated how to use the function, interpret results, and understand the role of bagging in creating robust model outputs. By exploring the importance scores and final predictions, you can gain insights into which predictors play a significant role and how bagging enhances stability and robustness in Lasso regression.
